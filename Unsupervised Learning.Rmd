---
title: "Unsupervised Learning"
author: "Laura Cline"
date: "13/11/2021"
output: 
  pdf_document:
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(ISLR)
library(leaps)
```

# Principal Components Analysis (PCA)

In this lab, we perform PCA on the `USArrests` data, which is part of the base `R` package. The rows of the dataset contain the 50 states, in alphabetical order. 

Objective: Perform PCA on the USArrests data, which is contained in the base `R` package. 

```{r}
states <- row.names(USArrests) 
states
```

The columns of the data contain four variables: `Murder`, `Assault`, `UrbanPop`, and `Rape`.

```{r}
names(USArrests)
```

We first briefly examine the data. We notice that the variables have vastly different means. 

```{r}
# Briefly examine the mean and variance of the four columns
apply(USArrests, 2, mean)
```

Note that the `apply()` function allows us to apply a function - in this case - `mean()` - to each row or column of the dataset. The second input here denotes whether we wish to compute the mean of the rows, `1`, or the columns `2`. We see that there are on average three times as many rapes as murders, and more than eight times as many assaults as rapes. We can also examine the variance of the four variables using the `apply()` function. 

```{r}
apply(USArrests, 2, var)
```

Not surprisingly, the variables also have vastly different variances: the `UrbanPop` variable measures the percentage of the population in each state living in an urban area, which is not a comparable number to the number of rapes in each state per 100,000 individuals. If we failed to scale the variables before performing PCA, then most of the principal components that we observed would be driven by the `Assault` variable, since it has by far the largest mean and variance. Thus, it is important to standardize the variables to have a mean of zero and standard deviation of one before performing PCA. 

First, notice how the `apply()` function is used - we are applying the `mean()` and `variance()` functions to the columns (second argument; 2) of the `USArrests` data. Second, observe the large difference in the means and variances of our variables. If we did not standardize the variables, the PCA ould mainly be driven by `Assault`. 

We now perform principal component analysis using the `prcomp()` function, which is one of several functions in `R` that perform PCA.

```{r}
# Perform principal component analysis usign the prcomp() function
pr.out <- prcomp(USArrests, scale = T) #prcomp() centers the variables to have mean zero by default, while scale = T scales the variables to have std.dec of 1 
```

By default, the `procomp()` function centers the variables to have a mean of zero. By using the option `scale = TRUE`, we scale the variables to have a standard deviation of one. The output from `prcomp()` contains a number of useful quantities. 

```{r}
# Center an scale components correspond to means and std. devs of the variables before implementing PCA
names(pr.out)
```

The `center` and `scale` components correspond to the means and standard deviations of the variables that were used for scaling prior to implementing PCA. 

```{r}
pr.out$center
```

```{r}
pr.out$scale
```

The `rotation` matrix provides the principal component loadings; each column of `pr.out$rotation` contains the corresponding principal component loading vector. This function names it the rotation matrix, because when we matrix-multiply the **X** matrix by `pr.out$rotation`, it gives us the coordinates of the data in the rotated coordinate system. These coordinates are the principal component score. 

```{r}
# The rotation matrix provides the principal component loading vectors
pr.out$rotation
```

Using the `prcomp()` function, we do not need to explicitly multiply the data by the principal component loading vectors in order to obtain the principal component score vectors. Rather the 50x4 matrix `x` has its columns the principal component score vectors. That is, the *k*th column is the *k*th principal component score vector. 

```{r}
# x contains the principal component score vectors
dim(pr.out$x)
```

We can plot the first two principal components as follows:

```{r}
biplot(pr.out, scale = 0)
```

The `scale = 0` argument to `biplot()` ensures that the arrows are scales to represent the loadings; other values for `scale` give slightly different biplots with different interpretations. 

Recall that the principal components are only unique up to a sign change. 

The `prcomp()` function also outputs the standard deviation of each principal component. For instance, on the `USArrests` dataset, we can access these standard deviations as follows:

```{r}
pr.out$sdev
```

The variance explained by each principal component is obtained by squaring these:

```{r}
# Find the amount of variance explained by each principal component 
pr.var <- pr.out$sdev^2
pr.var
```

To compute the proportion of variance explained by each principal component, we simply divide the variance explained by each principal component by the total variance explained by all four principal components:

```{r}
# To compute the proportion of variance explained by each PC, divide the variance explained by each PC by the total variance explained by all four PCs
pve <- pr.var / sum(pr.var)
pve
```

We see that the first principal component explains 62% of the variance in the data, the next principal component explains 24.7% of the variance, and so forth. We can plot the PVE explained by each component, as well as the cumulative PVE, as follows:

```{r}
# Plot the PVE of each component as well as the cumulative PVE
plot(pve, xlab = "Principal Component",
     ylab = "Proportion of Variance Explained",
     ylim = c(0,1),
     type = "b")
lines(cumsum(pve),
      type = "b",
      col="green")
```

Note that the function `cumsum()` computes the cumulative sum of the elements of a numeric vector. For instance:

```{r}
a = c(1, 2, 8, -3)
cumsum(a)
```

# Clustering 

## K-Means Clustering 

The function `kmeans()` performs *K*-means clustering in `R`. We begin with a simple simulated example in which these are truly two clusters in the data: the first 25 observations have a mean shift relative to the next 25 observations. 

Objective: Find the clusters of simulated data using the `kmeans()` function. 

```{r}
# Create a matrix containing two well-defined clusters
set.seed(2)
x <- matrix(rnorm(50*2), ncol=2)
x[1:25,1] <- x[1:25, 1] +3
x[1:25,2] <- x[1:25, 2] - 4
```

We now perform *K*-means clustering, with *K* = 2. 

```{r}
# Perform K-means clustering with K=2 and plot the results
km.out <- kmeans(x, 2, nstart=20)
```

The cluster assignments of the 50 observations are contained in `km.out$cluster`. 

```{r}
km.out$cluster
```

The *K*-means clustering perfectly separated the observations into two clusters even though we did not supply any group information to `kmeans()`. We can plot the data, with each observation coloured according to its cluster assignment. 

```{r}
plot(x, col = {km.out$cluster+1}, main = "K-means Clustering with K = 3",
     xlab = "", ylab = "", pch = 20, cex = 2)
```

Here the observations can be easily plotted because they are two-dimensional. If there were more than two variables then we could instead perform PCA and plot the first two principal components score vectors. 

In this example, we knew that there really were two clusters because we generated the data. However, for real data, in general we do not know the true number of clusters. We could instead have performed *K*-means clustering in this example with *K* = 3. 

```{r}
# To run the kmeans() function with multiple initial cluster assignments, use the nstart argument 
set.seed(4)
km.out <- kmeans(x, 3, nstart=20)
km.out
```

When *K* = 3, *K*-means clusterings splits up the two clusters. 

To run the `kmeans()` function in `R` with multiple initial cluster assignments, we use the `nstart` argument. If a value of `nstart` greater than one is used, then *K*-means clustering will be performed using multiple random assignments in Step 1 of the algorithm, and the `kmeans()` function will report only the best results. Here we compare using `nstart = 1` to `nstart = 20`. 

```{r}
set.seed(3)
km.out <- kmeans(x, 3, nstart = 1)
km.out$tot.withinss
```

```{r}
# Observe how this value is smaller than the previous result with only one initial set
set.seed(3)
km.out2 <- kmeans(x, 3, nstart=20)
km.out2$tot.withinss
```

Note that `km.out$tot.withinss` is the total within-cluster sum of squares, which we seek to minimize by performing *K*-means clustering. The individual within-cluster sum-of-squares are contained in the vector of `km.out$withinss`. 

We *strongly* recommend always running *K*-means clustering with a large value of `nstart`, such as 20 or 50, since otherwise an undesirable local optimum may be obtained. 

When performing *K*-means clustering, in addition to using multiple initial cluster assignments, it is also important to set a random seed using the `set.seed()` function. This way, the initial cluster assignments in Step 1 can be replicated, and the *K*-means output will be fairly reproducible. 

## Hierarchical Clustering 

The `hclust()` function implements hierarchical clustering in `R`. In the following example, we use data to plot the hierarchical clustering dendrogram using complete, single, and average clustering, with Euclidean distance as the dissimilarity measure. We begin by clustering observations using complete linkage. The `dist()` function is used to compute the 50x50 inter-observation Euclidean distance matrix. 

Objective: Use Euclidean distance as a dissimilarity measure to find clusters in the simulated data from the previous section. 

```{r}
hc.complete <- hclust(dist(x), method = "complete")
hc.single <- hclust(dist(x), method = "single")
hc.average <- hclust(dist(x), method = "average")
```

We can now plot the dendrograms obtained using the usual `plot()` function. The numbers at the bottom of the plot identify each observation. 

```{r}
# Plot the dendrograms for each clustering
par(mfrow=c(1,3))
plot(hc.complete, main = "Complete Linkage", cex = 0.9)
plot(hc.single, main = "Single Linkage", cex = 0.9)
plot(hc.average, main = "Average Linkage", cex = 0.9)

```

To determine the cluster labels for each observation associated with a given cut of the dendrogram, we can use the `cutree()` function:

```{r}
# Use cutree() to determine clusters associated with a given cut of a dendrogram tree
cutree(hc.complete, 2)
cutree(hc.single, 2)
cutree(hc.average, 2)
```

If there is a point that belongs to its own cluster, then it is probably necessary to increase the number of clusters. 

For this data, complete and average linkage generally separates the observations into their correct groups. However, single linkage identifies one point as beloning to its own cluster. A more sensible answer is obtained when four clusters are selected, although there are still two singletons. 

```{r}
cutree(hc.single, 4)
```

To scale the variables before performing hierarchical clustering of the observations, we use the `scale()` function:

```{r}
# Rerun hclust() with scaled variables
xsc <- scale(x)
plot(hclust(dist(xsc), method="complete"), main = "Hierarchical Clustering with Scales Features")
```

Correlation-based distance can be computed using the `as.dist()` function, which converts an arbitrary square symmetric matrix into a form that the `hclust()` function recognizes as a distance matrix. However, this only makes sense for data with at least three features since the absolute correlation between any two observations with measurements on two features is always 1. Hence, we will cluster a three-dimensional dataset. 

```{r}
# Practice clustering using a correlation-based distance measure
x <- matrix(rnorm(30*3), ncol=3)
dd <- as.dist(1 - cor(t(x)))
plot(hclust(dd, method="complete"), main = "Complete Linkage with Correlation-Based Distance")
```

# NCI60 Data Example

Unsupervised techniques are often used in the analysis of genomic data. In particular, PCA and hierarchical clustering are popular tools. We illustrate these techniques on the `NCI60` cancer cell line microarray data, which consists of 6,830 gene expression measurements on 64 cancer cell lines. 

```{r}
nci.labs = NCI60$labs
nci.data = NCI60$data
```

Each cell line is labeled with a cancer type. We do not make use of the cancer types in performing PCA and clustering, as these are unsupervised techniques. But after performing PCA and clustering, we will check to see the extent to which these cancer types agree with the results of these unsupervised techniques. 

The data has 64 rows and 6,830 columns. 

```{r}
dim(nci.data)
```

We begin by examining the cancer types for the cell lines. 

```{r}
nci.labs[1:4]
```

```{r}
table(nci.labs)
```

## PCA and the NCI60 Data

We first perform PCA on the data after scaling the variables (genes) to have a standard deviation of one, although one could reasonably argue that is is better not to scale the genes. 

```{r}
pr.out = prcomp(nci.data, scale = TRUE)
```

We now plot the first few principal component score vectors, in order to visualize the data. The observations (cell lines) corresponding to a given cancer type will be plotted in the same colour, so that we can see to what extent the observations within a cancer type are similar to each other. We first create a simple function that assigns a distinct colour to each element of a numeric vector. The function will be used to assign a colour to each of the 64 cell lines, based on the cancer type to which it corresponds. 

```{r}
cols = function(vec){
  cols = rainbow(length(unique(vec)))
  return(cols[as.numeric(as.factor(vec))])
}
```

Bote that the `rainbow()` function takes as its argument a positive integer aand returns a vector containing the number of distinct colours. We can now plot the principal component score vectors. 

```{r}
par(mfrow=c(1,2))
plot(pr.out$x[,1:2], col=cols(nci.labs), pch=19,
xlab="Z1",ylab="Z2")
plot(pr.out$x[,c(1,3)], col=cols(nci.labs), pch=19,
xlab="Z1",ylab="Z3")
```

On the whole, cell lines corresponding to a single cancer type do tent to have similar values on the first few principal components score vectors. This indicates that cell lines from the same cancer type tend to have pretty similar gene expression levels. 

We can also obtain a summary of the proportion of variance explained (PVE) of the first few principal components using the `summary()` method for a `prcomp` object:

```{r}
summary(pr.out)
```

Using the `plot()` function, we can also plot the variance explained by the first few principal components. 

```{r}
plot(pr.out)
```

Note that the height of each bar in the bar plot is given by squaring the corresponding element of `pr.out$sdev`. However, it is more informative to plot the PVE of each principal component (i.e., a scree plot) and the cumulative PVE of each principal component. This can be done with just a little work. 

```{r}
pve = 100*pr.out$sdev^2/sum(pr.out$sdev^2)
par(mfrow=c(1,2))
plot(pve, type="o", ylab = "PVE", xlab = "Principal Component", col="blue")
plot(cumsum(pve), type="o", ylab = "Cumulative PVE", xlab = "Principal Component", col="brown3")
```

Note that the elements of `pve` can also be computed directly from the summary, `summary(pr.out)$importance[2,]`, and the elements of `cumsum(pve)` are given by `summary(pr.out)$importance[3,]`.) We see that together, the first seven principal components explain around 40% of the variance in the data. This is not a huge amount of variance. However, looking at the scree plot, we see that while each of the first seven principal components explain a substantial amount of variance, there is a marked decrease in variance explained by further principal components. That is, there is an *elbow* in the plot after approximately the seventh principal component. This suggests that there may be little benefit to examining more than seven or so principal components (though even examining seven principal components may be difficult). 

## Clustering the Observations of the NCI60 Data

We now proceed to hierarchically cluster the cell lines in the `NCI60` data, with the goal of finding out whether or not the observations cluster into distinct types of cancer. To begin, we standardize the variables to have a mean of zero and a standard deviation of one. As mentioned earlier, this step is optional and should be performed only if we want each gene to be on the same *scale*. 

```{r}
sd.data = scale(nci.data)
```

We now perform hierarchical clustering of the observations usign complete, average and single linkage. Euclidean distance is used as the dissimilarity measure. 

```{r}
par(mfrow = c(1,3))
data.dist = dist(sd.data)
plot(hclust(data.dist), labels = nci.labs, main = "Complete Linkage", xlab = "", sub = "", ylab = "")
plot(hclust(data.dist, method = "average"), labels = nci.labs, main = "Average Linkage", xlab = "", sub = "", ylab = "")
plot(hclust(data.dist, method = "single"), labels = nci.labs, main = "Single Linkage", xlab = "", sub = "", ylab = "")
```

We see that the choice of linkage certainly does affect the results obtained. Typically, single linkage will tend to yield *trailing* clusters: very large clusters onto which individual observations attach one-by-one. On the other hand, complete and average linkage tend to yield more balanced, attractive clusters. For this reason, complete and average linkage are generally preferred to single linkage. Clearly, cell lines within a single cancer type do tend to cluster together, although the clustering is not perfect. We will use complete linkage hierarchical clustering for the analysis that follows. 

We can cut the dendrogram at the height that will yield a particular number of clusters, say four:

```{r}
hc.out = hclust(dist(sd.data))
hc.clusters = cutree(hc.out, 4)
table(hc.clusters, nci.labs)
```

There are some clear patterns. All the leukemia lines fall in cluster 3, while the breast cancer cell lines are spread out over three different clusters. We can plot the cut on the dendrogram that produces these four clusters. 

```{r}
par(mfrow=c(1,1))
plot(hc.out, labels = nci.labs)
abline(h=139, col="red")
```

The `abline()` function draws a stright line on top of any existing plot in `R`. The argument `h=139` plots the horizontal line at height 139 on the dendrogram; this is the height that results in four distinct clusters. It is easy to verify that the resulting clusters are the same ones we obtained using `cutree(hc.out, 4)`. 

Printing out the output of `hclust` gives a useful brief summary of the object:

```{r}
hc.out
```

We claimed earlier that *K*-means clusterign and hierarchical clusterign with the dendrogram cut to obtain the same number of clusters can yield different results. How do these `NCI60` hierarchical clustering results compare to what we get if we perform *K*-means clustering with *K* = 4?

```{r}
set.seed(2)
km.out = kmeans(sd.data, 4, nstart=20)
km.clusters = km.out$cluster
table(km.clusters, hc.clusters)
```

We see that the four clusters obtained using hierarchical clustering and *K*-means clustering are somewhat different. Cluster 4 in *K*-means clustering is identical to Cluster 3 in hierarchical clustering. However, the other clusters differ: for instance, Cluster 2 in *K*-means clustering contains a portion of the observations assigned to Cluster 1 by hierarchical clustering, as well as all of the observations to Cluster 2 by hierarchical clustering. 

Rather than performing hierarchical clustering on the entire data matrix, we can simply perform hierarchical clustering on the first few principal component score vectors, as follows:

```{r}
hc.out = hclust(dist(pr.out$x[,1:5]))
plot(hc.out, labels = nci.labs, main = "Hierarchical Clustering on \nFirst Five Score Vectors")
table(cutree(hc.out,4), nci.labs)

```

Not surprisingly, these results are different from the ones that we obtained when we performed hierarchical clustering on the full dataset. Sometimes performing clustering on the first few principal components score vectors can give better results than performing clustering on the full data. In this situation, we might view the principal component step as one of denoising the data. We count perform *K*-means clustering on the first few principal component score vectors rather than the full dataset. 

# Excercises 

## Question Two

Suppose we have four observations, for which we compute a dissimilarity matrix given by

[[, 0.3, 0.4, 0.7],
[0.3, , 0.5, 0.8], 
[0.4, 0.5, , 0.45], 
[0.7, 0.8, 0.45,]]

For instance, the dissimilarity between the first and second observations is 0.3, and the dissimilarity between the second the four observations is 0.8.

A. On the basis of this dissimilarity matrix, sketch the dendrogram that results from hierarchically clustering these four observations using complete linkage. Be sure to indicate on the plot the height at which each fusion occurs, as well as the observations corresponding to each leaf in the dendrogram. 

```{r}
set.seed(0)

DM = matrix(
  data = c(0.0,0.3,0.4,0.7,0.3,0.0,0.5,0.8,0.4,0.5,0.0,0.45,0.7,0.8,0.45,0.0), nrow = 4,
  ncol=4, byrow = TRUE
)
```

```{r}
plot(hclust(as.dist(DM), method="complete"), main = "Complete Linkage")
abline(h = 0.6, col = "red")
```

If we cut the dendrogram such that two clusters result, observations 1 & 2 will be in Cluster 1 and observations 3 and 4 will be in Cluster 2. 

B. Repeat (A), this time using single linkage clustering. 

```{r}
plot(hclust(as.dist(DM), method="single"), main = "Single Linkage")
abline(h = 0.44, col="red")
```

If we cut the dendrogram such that two clusters result, observation 4 will be in Cluster 1 and observations 3, 1, and 2 will be in Cluster 2. 

## Question Three

In this problem, you will perform *K*-meaans clustering manually, with *K* = 2, on a small example with *n* = 6 observations and *p* = 2 features.

A. Plot the observations

```{r}
set.seed(0)
DF = data.frame( x1=c(1,1,0,5,6,4), x2=c(4,3,4,1,2,0) )
```

```{r}
n = dim(DF)[1]
K = 2
```

B. Randomly assign a cluster label to each observation. You can use the `sample()` command in `R` to do this. Report the cluster labels for each observation. 

```{r}
labels = sample(1:K, n, replace=TRUE)
```

```{r}
plot(DF$x1, DF$x2, cex = 2, pch = 19, col=(labels+1), xlab = "gene index", ylab = "unpaired t-value")
grid()
```

C. Compute the centroid for each cluster

D. Assign each observation to the centroid to which it is closest, in terms of Euclidean distance. Report the cluster labels for each observation. 

E. Repeat (C) and (D) until the answers obtained stop changing.

```{r}
while(TRUE){
    # Part (c): Compute the centroids of each cluster
    cents = matrix(nrow=K, ncol=2)
    for(l in 1:K){
      samps = labels==l
      cents[l,] = apply( DF[samps,], 2, mean ) 
    }
    # Part (d): Assign each sample to the centroid it is closest too:
    new_labels = rep(NA, n)
    for(si in 1:n){
      smallest_norm = +Inf
      for(l in 1:K){
        nm = norm(as.matrix( DF[si,] - cents[l,]), type="2")
        if(nm < smallest_norm){
          smallest_norm = nm
          new_labels[si] = l
        }
      }
    }
    # Part (e): Repeat until labels stop changing:
    if(sum(new_labels == labels) == n){
      break
    }else{
      labels = new_labels
    }
}
```

F. In your plot from (A), colour the observations according to the cluster labels obtained. 

```{r}
plot(DF$x1, DF$x2, cex = 2, pch=19, col=(labels+1), xlab = "gene index", ylab = "unpaired t-value")
grid()
```

## Question Six

A researcher collects expression measurements for 1,000 genes in 100 tissue samples. The data can be written as a 1,000x100 matrix, which we call **X**, in which each row represents a gene and each column a tissue sample. Each tissue sample was processed on a different day, and the columns of **X** are ordered so that the samples that were processed earliest are on the left, and the samples that were processed later are on the right. The tissue samples belong to two groups: control (C) and treatment (T). The C and T samples were processed in a random order across the days. The researcher wishes to determine whether each gene's expression measurements differ between treatment and control groups. 

As a pre-analysis (before computing T versus C), the researcher performs principal component analysis on the data, and finds that the first principal component (a vector of length 100) has a strong linear trend from left to right, and explains 10% of the variation. The researcher now remembers that each patient sample was run on one of two machines, A and B, and machine A was used more often in earlier times while B was used more often later. The researcher has a record of which sample was run on each machine. 

C. Design and run a small simulation experiment to demonstrate the superiority of your idea.

```{r}
# Generate data
n = 1000 # the number of genes
m = 100 # The number of tissue samples
```

```{r}
mu_A = 0.1 # The machines are mostly the same but a bit different
sigma_A = 1

mu_B = -0.3
sigma_B = 4

mu_C = 0 # The control and treatment means are similiar
mu_T = 0.25
```

```{r}
X = matrix(0, nrow=n, ncol=m)
prob_of_machine_A = seq(1, 1.e-6, length.out=m)
machine = c()
treatment = c()

for(jj in 1:m){
    ## What machine did we use.  We slowly change from machine A to machine B.
    machine_used = sample(c('A', 'B'), size=1, prob=c(prob_of_machine_A[jj], 1-prob_of_machine_A[jj]))
    machine = c(machine, machine_used)

    ## Is this a control or a treatment sample: 
    ##
    type = sample(c('C', 'T'), size=1, prob=c(0.5, 0.5))
    treatment = c(treatment, type)

    if(machine_used=='A'){
        if(type=='C'){
            x = rnorm(n, mean=(mu_A+mu_C), sd=sigma_A)
        }else{
            x = rnorm(n, mean=(mu_A+mu_T), sd=sigma_A)
        }
    }else{
        if(type=='C'){
            x = rnorm(n, mean=(mu_B+mu_C), sd=sigma_B)
        }else{
            x = rnorm(n, mean=(mu_B+mu_T), sd=sigma_B)
        }
    }
    X[, jj] = x
}
```

```{r}
pr.out = prcomp(X, scale=TRUE)
```

```{r}
# Let's print the fraction of variance explained for the first 10 PCs:
print(pr.out$sdev[1:10]^2/sum(pr.out$sdev^2))
```

```{r}
# Perform the suggested transformation:
X_transformed = X - pr.out$x[,1] %*% t(pr.out$rotation[, 1])
```

```{r}
# Run the T-Test:
print(t.test(X_transformed[, treatment=='C'], X_transformed[, treatment=='T']))
```

```{r}
# Split into two groups, normalize, and recombine:
machine_A = machine=='A'
X_A = X[, machine_A]
    
machine_B = machine=='B'
X_B = X[, machine_B]

print(sprintf('mu_A = %f; mean(X_A) = %f; mu_B = %f; mean(X_B) = %f', mu_A, mean(X_A), mu_B, mean(X_B)))
```

```{r}
X_2 = cbind(X_A - mean(X_A), x_B = mean(X_B))
```

```{r}
#pr.out = prcomp(X_2, scale=TRUE)
```

```{r}
# Let's print out the fraction of variance explained for the first 10 PCs:
#print(pr.out$sdev[1:10]^2/sum(pr.out$sdev^2))
```

```{r}
# Run the T-Test:
#print(t.test(X_2[, treatment=='C'], X_2[, treatment=='T']))
```

## Question Seven 

In the chapter, we mentioned the use of correlation-based distance and Euclidean distance as dissimilarity measures for hierarchical clustering. It turns out that these two measures are almost equivalent: if each observation has been centered to have a mean of zero and a standard deviation of one, and if we let $r_{ij}$ denote the correlation between the *i*th and *j*th observations, then the quantity of $1 - r_{ij}$ is proportional to the squared Euclidean distance between the *i*th and *j*th observations. 

On the `USArrests` data, show that this proportionality holds. 

The Euclidean distance can be calculated usign the `dist()` function, and the correlations can be calculated using the `cor()` function. 

```{r}
set.seed(0)
```

```{r}
# Scale each observation (not the features):
USA_scaled = t(scale(t(USArrests)))
```

```{r}
# The correlation of each sample with the other samples
Rij = cor(t(USA_scaled)) # -1 <= Rij <= +1
OneMinusRij = 1 - Rij # 0 <= 1 - Rij <= +2
X = OneMinusRij[lower.tri(OneMinusRij)]
```

```{r}
D = as.matrix(dist(USA_scaled)^2)
Y = D[lower.tri(D)]
```

```{r}
plot(X,Y)
```

```{r}
summary(X/Y)
```

## Question Eight 

One the `USArrests` data, calculate PVE in two ways:

A. Using the `sdev` output of the `prcomp()` function. 

```{r}
pr.out = prcomp(USArrests, scale=TRUE)
```

```{r}
# Using the output from pr
pr.var = pr.out$sdev^2
pve_1 = pr.var/sum(pr.var)
pve_1
```

B. Use the `prcomp()` function to compute the principal component loadings. Then, use the loadings to obtain the PVE. 

```{r}
USArrests_scaled = scale(USArrests)
denom = sum(apply(USArrests_scaled^2, 2, sum))
```

```{r}
Phi = pr.out$rotation
USArrests_projected = USArrests_scaled %*% Phi # This is the same as pr.out$x
```

```{r}
numer = apply(pr.out$x^2, 2, sum)
pve_2 = numer /denom
pve_2
```

## Question Nine 

Consider the `USArrests` data. We will now perform hierarchical clustering on the states. 

A. Using hierarchical clustering with complete linkage and Euclidean distance, cluster the states. 

B. Cut the dendrogram at a height that results in three distinct clusters. What states belong to which clusters?

```{r}
hclust.complete = hclust(dist(USArrests), method="complete")
```

```{r}
plot(hclust.complete, xlab = "", sub = "", cex = 0.9)
```

```{r}
ct = cutree(hclust.complete, k = 3) # number of clusters to cut into
```

```{r}
# Print which states go into each cluster:
for(k in 1:3){
  print(k)
  print(rownames(USArrests)[ct == k])
}
```

C. Hierarchically cluster the states using complete linkage and Euclidean distance, *after scaling the variables to have a standard deviation of one*. 

```{r}
hclust.complete.scale = hclust(dist(scale(USArrests, center = FALSE)), method="complete")
```

```{r}
plot(hclust.complete.scale, xlab = "", sub = "", cex = 0.9)
```

```{r}
ct = cutree(hclust.complete.scale, k =3) # number of clusters to cut into
```

```{r}
# Print which states go into each cluster in this case:
for(k in 1:3){
  print(k)
  print(rownames(USArrests)[ct == k])
}
```

## Question Ten

In this problem, you will generate simulated data, and then perform PCA and K-means clustering on the data. 

A. Generate a simulated dataset with 20 observations in each of three classes (i.e., 60 observations total), and 50 variables. 

```{r}
# Generate data
K = 3 # The number of classes
n = 20 # the number of samples per class
p = 50 # the number of variables 
```

```{r}
# Create data for class 1:
X_1 = matrix(rnorm(n*p), nrow=n, ncol=p)
for(row in 1:n){
  X_1[row,] = X_1[row,] + rep(1,p)
}
```

```{r}
# Create the data for class 2:
X_2 = matrix(rnorm(n*p), nrow=n, ncol=p)
for(row in 1:n){
  X_2[row,] = X_2[row,] + rep(-1,p)
}
```

```{r}
# Create data for class 3:
X_3 = matrix(rnorm(n*p), nrow=n, ncol=p)
for(row in 1:n){
  X_3[row,] = X_3[row,] + c(rep(+1, p/2), rep(-1, p/2))
}
```

```{r}
X = rbind(X_1, X_2, X_3)
labels = c(rep(1,n), rep(2,n), rep(3,n)) # the "true" labels of the points 
```

B. Perform PCA on the 60 observations and plot the first two principal component score vectors. Use a different colour to indicate the observations in each of the three classes. If the three classes appear separated in this plot, then continue on to part (C). If not, then return to part (A) and modify the simulation so that there is a greater separation between the three classes. Do not continue to part (C) until the three classes show at least some separation in the first two principal component score vectors. 

```{r}
pr.out = prcomp(X, scale=TRUE)
```

```{r}
plot(pr.out$x[,1], pr.out$x[,2], col=labels, pch=19)
grid()
```

C. Perform *K*-means clustering of the observations with *K* = 3. How well do the clusters that you obtained in *K*-means clustering compare to the true class labels?

You can use the `table()` function in `R` to compare the true class labels to the class labels obtained by clustering. Be careful how you interpret the results: K-means clustering will arbitrarily number the clusters, so you cannot somply check whether the true class labels and clustering labels are the same. 

```{r}
kmean.out = kmeans(X, centers=3, nstart=50)
```


```{r}
table(kmean.out$cluster, labels)
```

D. Perform *K*-means clustering with K=2.

```{r}
kmean.out = kmeans(X, centers=2, nstart=50)
table(kmean.out$cluster, labels)
```

E. Now perform *K*-means clustering with *K* = 4. 

```{r}
kmean.out = kmeans(X, centers=4, nstart=50)
table(kmean.out$cluster, labels)
```

F. Now perform *K*-means clustering with *K*=3 on the first two principal component score vectors, rather than on the raw data. That is, perform *K*-means clustering on the 60x2 matrix of which the first column is the first principal component score vector, and the second column is the second principal component score vector. 

```{r}
kmean.out = kmeans(pr.out$x[,c(1,2)], centers=3, nstart=50)
table(kmean.out$cluster, labels)
```

G. Using the `scale()` function, perform *K*-means clustering with *K*=3 on the data *after scaling each variable to have standard deviation one*. 

```{r}
Xs = scale(X)
kmean.out = kmeans(Xs, centers=3, nstart=50)
table(kmean.out$cluster, labels)
```

## Question Eleven

There is a gene expression dataset `Ch10Ex11.csv` that consists of 40 tissue samples with measurements on 1,000 genes. The first 20 samples are from healthy patients, while the second 20 are from the diseased group. 

A. Load the data using `read.csv()`. You will need to select `header=F`. 

```{r}
DF = read.csv("https://raw.githubusercontent.com/asadoughi/stat-learning/master/ch10/Ch10Ex11.csv", header=FALSE)
DF = t(DF) # want each row to represent a sample...should have n=40 samples/rows
```

B. Apply hierarchical clustering to the samples using correlation-based distance, and plot the dendrogram. Do the genes separate the samples into two groups? Do your results depend on the type of linkage used?

```{r}
D = dist(DF) #n*n matrix of Euclidean distance dissimilarities 
D = as.dist(1 - cor(t(DF))) # cor computes the correlation of columns so we need to take the transpose of DF
hclust.cor = hclust(D, method="complete")
#hclust.cor = hclust(D, method = "average")
#hclust.cor = hclust(D, method="single")
```

```{r}
# How well does our clustering predict health vs. diseased
print(table(predicted = cutree(hclust.cor, k=2), truth = c(rep(0,20), rep(1,20))))
```

```{r}
plot(hclust.cor, xlab = "", sub = "", cex=0.9)
```

C. Your collaborator wants to know which genes differ the most across the two groups. Suggest a way to answer this question, and apply it here. 

```{r}
# Compute the unpaired t-test between the means of the gene response in each cluster:
predicted = cutree(hclust.cor, k=2)
```

```{r}
n1 = apply(DF[predicted==1,], 2, length) # the number of samples (number of patients in each cluster)
n2 = apply(DF[predicted==2,], 2, length)
```

```{r}
m1 = apply(DF[predicted==1,], 2, mean) # the means across the 1000 genes in each cluster
m2 = apply(DF[predicted==2,], 2, mean)
```

```{r}
v1 = apply(DF[predicted==1,], 2, var) #the variances across the 1000 genes in each cluster
v2 = apply(DF[predicted==2,], 2, var)
```

```{r}
pooled_variance = sqrt(v1 / n1 + v2 / n2)
```

```{r}
t_value = (m1 - m2) / pooled_variance
```

```{r}
plot(t_value, xlab = "gene index", ylab = "unpaired t-value")
```

